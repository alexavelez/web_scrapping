{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ca98e3-0485-4f43-98f3-14dc29e8148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11f6f1-d945-4faf-be26-3783820d069c",
   "metadata": {},
   "source": [
    "Creating a function to extract all the links for the datasets in the webpage\n",
    "\n",
    "The links to each dataset are inside <a> elements that do not have any class or ids. They are nested inside h3, div and li elements that have a specific class and I will use to target the <a> elements.\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "\n",
    "This line starts an except block.\n",
    "\n",
    "requests.exceptions.RequestException is a base class for exceptions raised by the requests library during network requests. This includes various exceptions like:\n",
    "\n",
    "ConnectionError: Problems with the network connection.\n",
    "Timeout: Request timed out.\n",
    "TooManyRedirects: Too many redirects encountered.\n",
    "HTTPError: HTTP error responses (e.g., 404 Not Found, 500 Server Error).\n",
    "as e: This assigns the specific exception instance to the variable e. This allows me to access information about the specific error that occurred.\n",
    "\n",
    "print(f\"Error fetching URL: {e}\")\n",
    "\n",
    "This line prints an informative error message to the console.\n",
    "f-string (formatted string literal) is used to create the message dynamically.\n",
    "{e} is a placeholder that gets replaced with the actual error object (e).\n",
    "return []\n",
    "\n",
    "This line returns an empty list ([]).\n",
    "If an exception occurs during the request, the function will return an empty list instead of the expected list of links.\n",
    "This helps to handle the error gracefully and prevent the program from crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca9528f-88ab-4a1a-9dba-7e29178b7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_links(url):\n",
    "  \"\"\"\n",
    "  Extracts all href attributes from <a> tags within <h3 class=\"dataset_heading\"> \n",
    "  elements on the given webpage.\n",
    "\n",
    "  Args:\n",
    "    url: The URL of the webpage to scrape.\n",
    "\n",
    "  Returns:\n",
    "    A list of href attributes extracted from the <a> tags.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for dataset_item in soup.find_all('li', class_='dataset-item has-organization'):\n",
    "      dataset_content = dataset_item.find('div', class_='dataset-content')\n",
    "      if dataset_content:\n",
    "        for dataset_heading in dataset_content.find_all('h3', class_='dataset-heading'):\n",
    "          for link in dataset_heading.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            links.append(href)\n",
    "\n",
    "    return links\n",
    "\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching URL: {e}\")\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff26e6-0685-4856-8297-7302bb1fe119",
   "metadata": {},
   "source": [
    "Create a function to shorten the descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019395c8-9d59-40a3-af88-7960c97dc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_description(description, max_words=30):\n",
    "  \"\"\"Shortens a description to the specified word limit.\"\"\"\n",
    "  words = description.split()\n",
    "  if len(words) > max_words:\n",
    "    description = ' '.join(words[:max_words]) + '...'\n",
    "  return description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ede68-9dbb-4772-8a98-f230e3611814",
   "metadata": {},
   "source": [
    "Creating a Function to Extract Information from Each Dataset URL:\n",
    "\n",
    "Parse the Title: Extracts the title of the dataset.\n",
    "Parse the Description: Extracts the dataset description and passes it to the shorten_description function.\n",
    "Parse Additional Information: Extracts all other relevant details from the dataset's table.\n",
    "Since not all datasets contain the same information, the function assigns \"Not available\" for any missing fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5579fb98-4ae7-46ae-8b6f-2007a73fa423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_dataset_details(dataset_url):\n",
    "  \"\"\"\n",
    "  Fetches and parses the details page of a given dataset URL.\n",
    "\n",
    "  Args:\n",
    "    dataset_url: The URL of the dataset page to scrape.\n",
    "\n",
    "  Returns:\n",
    "    A dictionary containing the extracted dataset details (e.g., title, description, etc.).\n",
    "  \"\"\"\n",
    "  try:\n",
    "    response = requests.get(dataset_url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract specific details from the dataset page (customize as needed)\n",
    "    dataset_name = soup.find('h1', attrs={'itemprop': 'name'}).text.strip()  # Example: Extract title from <h1> tag\n",
    "      \n",
    "    description_div = soup.find('div', itemprop=\"description\", class_=\"notes embedded-content\")\n",
    "    if description_div:\n",
    "      description_p = description_div.find('p')\n",
    "      if description_p:\n",
    "        description = description_p.text.strip()\n",
    "        short_description = shorten_description(description, max_words=30)\n",
    "\n",
    "    # Extract information from table \n",
    "    table_dict= {}\n",
    "    tables = soup.find_all('table', class_='table table-striped table-bordered table-condensed')\n",
    "\n",
    "    # Ensure there are tables on the page\n",
    "    if len(tables) < 2:\n",
    "        print(\"Table 2 not found on the page.\")\n",
    "    else:\n",
    "    # Select the second table (index 1)\n",
    "        table = tables[1]\n",
    "\n",
    "    # Extract rows from the table and store in a dictionary\n",
    "\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        th = row.find('th')  # Find the header\n",
    "        td = row.find('td')  # Find the corresponding value\n",
    "        if th and td:\n",
    "            th_text = th.get_text(strip=True)\n",
    "            td_text = td.get_text(strip=True)\n",
    "            table_dict[th_text] = td_text\n",
    "\n",
    "    resource_type = table_dict.get(\"Resource Type\", \"Not available\")\n",
    "    metadata_created_date = table_dict.get(\"Metadata Created Date\", \"Not available\")\n",
    "    identifier = table_dict.get(\"Identifier\", \"Not available\")\n",
    "    doi = table_dict.get(\"DOI\", \"Not available\")\n",
    "    publisher =  table_dict.get(\"Publisher\", \"Not available\")\n",
    "    maintainer = table_dict.get(\"Maintainer\", \"Not available\")\n",
    "    a_id = table_dict.get(\"@Id\", \"Not available\")\n",
    "    data_last_modified = table_dict.get(\"Data Last Modified\", \"Not available\")\n",
    "    public_access = table_dict.get(\"Public Access Level\", \"Not available\")  \n",
    "    data_first_published = table_dict.get(\"Data First Published\", \"Not available\")\n",
    "\n",
    "    \n",
    "    data_dict = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'description': short_description,\n",
    "        'resource_type': resource_type,\n",
    "        'metadata_created_date': metadata_created_date,\n",
    "        'identifier': identifier,\n",
    "        'doi': doi,\n",
    "        'publisher': publisher,\n",
    "        'maintainer': maintainer,\n",
    "        'a_id': a_id,\n",
    "        'data_last_modified': data_last_modified,\n",
    "        'public_access': public_access,\n",
    "        'data_first_published': data_first_published,\n",
    "        }\n",
    "\n",
    "    data.append(data_dict)\n",
    "\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching dataset URL: {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c64fe9-c23c-4691-b680-d49ff78b51b7",
   "metadata": {},
   "source": [
    "Loop Through Pages Using Pagination Parameters:\n",
    "Implement a loop to navigate through the pages using pagination parameters. The loop continues until no new data is added when it finds an empty webpage. This website doesn't implement a 404 page after the last page with results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae75bbb-8709-49bd-82d5-6741b67d79bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through pages and scrape dataset links with the get_dataset_links function created before.\n",
    "current_page = 1\n",
    "all_links = []\n",
    "\n",
    "while True:\n",
    "    url = f\"https://catalog.data.gov/dataset/?tags=machine-learning&page={current_page}\"\n",
    "    dataset_links = get_dataset_links(url)\n",
    "\n",
    "    if dataset_links:  # If links are found, add them to the list and move to the next page\n",
    "        all_links.extend(dataset_links)\n",
    "        current_page += 1\n",
    "        time.sleep(2)\n",
    "    else:  # Stop the loop if no links are found\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2612c56-2c3a-40e1-b7ab-a22a1641adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop trough all the links and scrape the information for each of the datasets or studies.\n",
    "for link in all_links:\n",
    "    dataset_link = \"https://catalog.data.gov\" + str(link)\n",
    "    dataset_details = scrape_dataset_details(dataset_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4912ec79-9856-4fb3-88e2-f12f51c397b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the information to a csv file.\n",
    "import csv\n",
    "\n",
    "def list_of_dicts_to_csv(data_list, filename):\n",
    "  \"\"\"\n",
    "  Converts a list of dictionaries to a CSV file.\n",
    "\n",
    "  Args:\n",
    "    data_list: A list of dictionaries.\n",
    "    filename: The name of the CSV file to create.\n",
    "  \"\"\"\n",
    "  with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "    fieldnames = data_list[0].keys()  # Get the header row from the first dictionary\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_list)\n",
    "\n",
    "list_of_dicts_to_csv(data, 'datagov_ml.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
